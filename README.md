# Union Budget RAG Backend

A production-ready RAG (Retrieval-Augmented Generation) system for document-based question answering with history-aware retrieval and multi-language support.

## ğŸŒ Live Demo

**Hosted Backend API**: [https://hcl-test.onrender.com](https://hcl-test.onrender.com)

- Health Check: [https://hcl-test.onrender.com/api/health](https://hcl-test.onrender.com/api/health)
- API Base URL: `https://hcl-test.onrender.com/api`


## Architecture

### System Overview

```
Client (Frontend)
    â†“ HTTP/REST
Flask API Server
    â”œâ”€â”€ Auth Routes (JWT)
    â”œâ”€â”€ Chat Routes (RAG Pipeline)
    â””â”€â”€ Ingest Routes (Document Processing)
    â†“
PostgreSQL (Users, Chats, Messages) + Pinecone (Vector Store)
```

### RAG Pipeline Flow

```
1. User Message
   â†“
2. Language Detection â†’ Translate to English (if needed)
   â†“
3. Load Chat History (last 8 messages from PostgreSQL)
   â†“
4. History-Aware Query Rewriting
   - Uses LLM to contextualize ambiguous queries
   - Example: "What about infrastructure?" â†’ "What is the infrastructure budget in Union Budget 2026-27?"
   â†“
5. Vector Similarity Search (Pinecone)
   - Generate query embedding (HuggingFace)
   - Retrieve top-K documents (default: 6)
   â†“
6. Response Generation (Gemini LLM)
   - Context: Retrieved documents + Chat history
   - Language: Original user language
   â†“
7. Persist Messages (PostgreSQL)
   - Save user message
   - Save assistant response with sources
   â†“
8. Return Response
```

### High-level Architecture
![Architecture](./hld.jpeg)

### History-Aware Retrieval

**Problem**: Follow-up questions are ambiguous without context.

**Solution**: Query rewriting using chat history.

- **Input**: Chat history + Current query
- **Process**: LLM rewrites query to be standalone and contextual
- **Output**: Improved retrieval accuracy

**Example**:
- Q1: "What is the healthcare budget?"
- A1: "The healthcare budget is $50 billion..."
- Q2: "What about infrastructure?" 
- **Rewritten**: "What is the infrastructure budget allocation in the Union Budget 2026-27?"

### Document Ingestion Flow

```
PDF File
  â†“
Text Extraction (PyMuPDF, page-by-page)
  â†“
Text Chunking (400 chars, 80 overlap)
  â†“
Embedding Generation (HuggingFace all-MiniLM-L6-v2, 384-dim)
  â†“
Vector Storage (Pinecone with metadata)
```

### Component Architecture

**Core Components**:
- `app/rag/retriever.py` - Vector similarity search
- `app/rag/history_aware.py` - Contextual query rewriting
- `app/rag/generator.py` - LLM response generation
- `app/services/language_service.py` - Multi-language detection & translation
- `app/services/embeddings.py` - HuggingFace embeddings
- `app/services/pinecone_client.py` - Vector database operations

**Data Flow**:
- **PostgreSQL**: User authentication, chat sessions, message history
- **Pinecone**: Document embeddings with metadata (doc_name, page_number, chunk_index)
- **Gemini LLM**: Query rewriting, response generation, translation

## Setup

### Prerequisites

- Python 3.10+
- PostgreSQL (Neon recommended)
- Pinecone account
- Google AI Studio API key

### Installation

1. **Install dependencies**:
   ```bash
   cd backend
   pip install -r requirements.txt
   ```

2. **Configure environment** (create `.env`):
   ```bash
   DATABASE_URL=postgresql://user:password@host/dbname?sslmode=require
   PINECONE_API_KEY=your-pinecone-api-key
   PINECONE_INDEX_NAME=union-budget-rag
   GOOGLE_API_KEY=your-gemini-api-key
   JWT_SECRET=your-super-secret-key-min-32-chars
   JWT_EXPIRY_DAYS=7
   RAG_TOP_K=6
   CHAT_HISTORY_LIMIT=8
   PDF_SOURCE_DIR=./docs
   FLASK_PORT=4000
   ```

3. **Run database migrations**:
   ```bash
   python migrate.py
   ```

4. **Ingest documents**:
   ```bash
   # Add PDFs to docs/ folder
   python ingest.py
   ```

5. **Start server**:
   ```bash
   python run.py
   ```

Server runs at `http://localhost:4000`

## ğŸŒ Production Deployment

The backend is deployed on **Render**:
- **Live API**: [https://hcl-test.onrender.com](https://hcl-test.onrender.com)
- **Health Check**: [https://hcl-test.onrender.com/api/health](https://hcl-test.onrender.com/api/health)

## Tech Stack

- **Framework**: Flask
- **Database**: PostgreSQL (Neon) + SQLAlchemy ORM
- **Vector DB**: Pinecone
- **RAG Framework**: LangChain
- **Embeddings**: HuggingFace (all-MiniLM-L6-v2, 384-dim)
- **LLM**: Google Gemini 2.0 Flash
- **Auth**: JWT + bcrypt
- **PDF Parsing**: PyMuPDF

## Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                 # Flask application
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ routes/                 # API endpoints
â”‚   â”œâ”€â”€ rag/                    # RAG pipeline components
â”‚   â”œâ”€â”€ models/                 # Database models
â”‚   â”œâ”€â”€ services/               # Business logic (PDF, embeddings, etc.)
â”‚   â”œâ”€â”€ auth/                   # JWT & password handling
â”‚   â””â”€â”€ db/                     # Database session & migrations
â”œâ”€â”€ docs/                       # PDF upload directory
â”œâ”€â”€ ingest.py                   # CLI ingestion script
â”œâ”€â”€ migrate.py                  # Database migration script
â””â”€â”€ run.py                      # Run API server
```

## Key Features

- **History-Aware Retrieval**: Contextualizes queries using chat history
- **Multi-Language Support**: Detects and responds in 13+ Indian languages
- **Document Ingestion**: Automatic PDF processing with idempotency
- **JWT Authentication**: Secure user authentication
- **Chat Persistence**: PostgreSQL-based chat history
